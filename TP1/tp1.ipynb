{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Fouille de donn√©es\n",
    "\n",
    "\n",
    "## TP1 Automne 2022 - Duplicate Bug Report Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Due date: 02/10\n",
    "\n",
    "##### Team Members:\n",
    "\n",
    "    - Mohammed Amin, Nawras (1962832)\n",
    "    - Patel, Pritam (1933097)\n",
    "    - Ariful Islam, Mohammed (19XXXXX)\n",
    "    \n",
    "##### Deliverables:\n",
    "\n",
    "You must submit two separate files to Moodle:\n",
    "1. Notebook\n",
    "2. [Json](https://en.wikipedia.org/wiki/JSON) containing scraped web-page content (bug_reports.json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - R√©sum√©/Introduction\n",
    "\n",
    "En raison de la complexit√© des syst√®mes logiciels, les bogues logiciels sont r√©pandus. Les entreprises, en particulier les grandes, utilisent g√©n√©ralement un syst√®me de suivi des bogues (BTS), √©galement appel√© syst√®me de suivi des probl√®mes, pour g√©rer et suivre les enregistrements des bogues. Outre les d√©veloppeurs et les testeurs, de nombreux projets, principalement des projets open source, permettent aux utilisateurs de signaler de nouveaux bogues dans leur BTS. Pour ce faire, les utilisateurs doivent remplir un formulaire avec plusieurs champs. Un sous-ensemble important de ces champs fournissent des donn√©es cat√©gorielles et n'acceptent que les valeurs qui vont d'une liste fixe d‚Äôoptions (par exemple, composant, version et produit du syst√®me). Deux autres domaines importants sont le r√©sum√© et la description. Les utilisateurs sont libres d'√©crire quoi que ce soit dans les deux champs et la seule contrainte est que le r√©sum√© a un nombre maximum de caract√®res. La soumission d'un formulaire cr√©e une page, appel√©e rapport de bogue ou rapport de probl√®me, qui contient toutes les informations sur un bogue.\n",
    "\n",
    "En raison du manque de communication et de synchronisation, les utilisateurs peuvent ne pas savoir qu'un bogue sp√©cifique a d√©j√† √©t√© soumis et le signaler √† nouveau. Identifier les rapports de bogues en double est une t√¢che importante dans les BTS et c'est le sujet de ce TP. Fondamentalement, notre objectif est de d√©velopper un syst√®me qui pr√©dit si une paire de nouveau rapport de bogue et un rapport de bogue soumis sont dupliqu√© ou non. Ce syst√®me sera utilis√© pour identifier manuellement les rapports dupliqu√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "Due to complexities of software systems, software bugs are prevalent. Companies, especially large ones, usually use a Bug Tracking System (BTS), also called Issue Tracking System, to manage and track records of bugs. Besides developers and testers, many projects, mainly open source projects, allow users to report new bugs in their BTS.\n",
    "To do that, users have to fill a form with multiple fields. An important subset of\n",
    "these fields provides categorical data and only accepts values that range from a fixed list of\n",
    "options (e.g., component, version and product of the system). Another two important fields\n",
    "are the summary and the description. The users are free to write anything in both fields\n",
    "and the only constraint is that the summary has a maximum number of characters. The\n",
    "submission of a form creates a page, called bug report or issue report, which contains all\n",
    "the information about a bug.\n",
    "\n",
    "Due to the lack of communication and syncronization, users may not be aware that a specific bug was already submitted and report it again. Identifying duplicate bug reports is important task in the BTSs and it is the subject of this TP. Basically, our goal is to develop a system that will predicte wheather a pair of new bug report and submitted one is duplicate or not. This system will be used to manually identify duplicate reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce TP, vous aurez besoin des librairies `numpy` et `sklearn`, ainsi que la librairie `nltk`, qui est une libraire utilis√©e pour faire du traitement du language (Natural Language Processing, NLP)\n",
    "\n",
    "Installez les libraires en question et ex√©cutez le code ci-dessous :\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can use anaconda and install after nltk library\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} numpy\n",
    "!conda install --yes --prefix {sys.prefix} scikit-learn\n",
    "!conda install --yes --prefix {sys.prefix} nltk\n",
    "!conda install --yes --prefix {sys.prefix} requests\n",
    "\n",
    "#python\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Les donn√©es / Data\n",
    "\n",
    "T√©l√©chargez l'archive √† l'adresse suivante: https://www.dropbox.com/s/s53fqz29z8ch4ip/data.zip?dl=0\n",
    "\n",
    "In this zip file, there are: \n",
    "\n",
    "1. training.txt : ce fichier contient des paires de rapports de bogues qui seront utilis√©s pour entra√Æner notre syst√®me.\n",
    "2. validation.txt : Ce fichier contient des paires de rapports de bogues qui seront utilis√©s pour √©valuer notre syst√®me.\n",
    "2. bug_reports : Ce dossier contient le code HTML des bug reports. Chaque fichier HTML est nomm√© selon le motif **bug_report_id.html**.\n",
    "\n",
    "\n",
    "L'image ci-dessous illustre un exemple de bug report:\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "Please download the zip file in the following url: https://www.dropbox.com/s/s53fqz29z8ch4ip/data.zip?dl=0\n",
    "\n",
    "In this zip file, there are: \n",
    "\n",
    "1. training.txt: This file contains pairs of bug reports that will be used to train our system.\n",
    "2. validation.txt: This file contains  pairs of bug reports that will be used to evaluate our system.\n",
    "2. bug_reports: It is a folder that contains the bug report html source. Each html file name follows the pattern **bug_report_id.html**.\n",
    "\n",
    "\n",
    "Figure below depicts an bug report page example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Image(\"https://irving-muller.github.io/images/bug-report-eclipse.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A : identifiant du bug report\n",
    "- B : date de cr√©ation\n",
    "- C : r√©sum√©\n",
    "- D : produit\n",
    "- E : composant\n",
    "- F : l'identifiant du rapport dont le bug report est dupliqu√©\n",
    "- G : description\n",
    "\n",
    "\n",
    "Le script suivant charge le jeu de donn√©es de test et d√©finit certaines variables globales:\n",
    "\n",
    "--------\n",
    "\n",
    "\n",
    "- A : bug report id\n",
    "- B : creation date\n",
    "- C : summary\n",
    "- D : product\n",
    "- E : component\n",
    "- F : the report id which the bug report is duplicate\n",
    "- G : description\n",
    "\n",
    "The following script loads the test dataset and define some global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# d√©finir le chemin du dossier qui contient les donn√©es\n",
    "FOLDER_PATH = \"data/\"\n",
    "PAGE_FOLDER = os.path.join(FOLDER_PATH, 'bug_reports')\n",
    "\n",
    "\n",
    "# Charger l'ensemble de donn√©es d'√©valuation \n",
    "import json\n",
    "\n",
    "training_file = open(os.path.join(FOLDER_PATH, \"training.txt\"))\n",
    "validation_file = open(os.path.join(FOLDER_PATH, \"validation.txt\"))\n",
    "word_vec_path = os.path.join(FOLDER_PATH, \"glove.42B.300d_clear.txt\")\n",
    "\n",
    "def read_dataset(f):\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        \n",
    "        rep1, rep2, label = line.split(',')\n",
    "\n",
    "        rep1 = int(rep1)\n",
    "        rep2 = int(rep2)\n",
    "        label = 1.0 if int(label) > 0 else 0.0 \n",
    "        \n",
    "        yield (rep1, rep2, label)\n",
    "    \n",
    "\n",
    "training_pairs = list(read_dataset(training_file))\n",
    "validation_pairs = list(read_dataset(validation_file))\n",
    "\n",
    "training_reports_set = set()\n",
    "\n",
    "for r1, r2, _ in training_pairs:\n",
    "    training_reports_set.add(r1)\n",
    "    training_reports_set.add(r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Web scraping\n",
    "\n",
    "\"Le *web scraping* (parfois appel√© harvesting) est une technique d'extraction du contenu de sites Web, via un script ou un programme, dans le but de le transformer pour permettre son utilisation dans un autre contexte, par exemple le r√©f√©rencement.\" [Wikipedia](https://fr.wikipedia.org/wiki/Web_scraping)\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "Web scraping consists in extracting relevant data from pages and prepare it for computational analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Question 1 (4 points)\n",
    "\n",
    "**Impl√©mentez la fonction ```extract_data_from_page``` qui extrait les informations suivantes du code HTML: l'ID du bug report, la date de cr√©ation, le titre, le produit, le composant, l'ID du bug report dont il est un duplicata, et la description.**\n",
    "\n",
    "La fonction ```extract_data_from_page``` retourne un dictionnaire avec la structure suivante:\n",
    "\n",
    "```python\n",
    " {\"report_id\": int, \n",
    "  \"dup_id\": int or None (the report id which it is duplicate), \n",
    "  \"component\": string, \n",
    "  \"product\": string, \n",
    "  \"summary\": str, \n",
    "  \"description\": string, \n",
    "  \"creation_date\": string} \n",
    "```\n",
    "\n",
    "Par exemple, pour le bug report \"bug_report/18431.html\", la fonction *extract_data_from_page* retourne:\n",
    " \n",
    "```python\n",
    "{'report_id': 18431,\n",
    " 'dup_id': 27227,\n",
    " 'component': 'GEF-Legacy Draw2d',\n",
    " 'product': 'GEF',\n",
    " 'summary': 'Polylines ignored by FLowLayout',\n",
    " 'description': 'I tried a poor sample but it\\'s not working the way I thought it would be. Look \\nat the following source. I have a figure with FlowLayout and add 2 polylines to \\nit but the polylines aren\\'t arranged in FlowLayout. Both polylines are pushed \\ninto the first flow element. Any ideas why?\\n\\n\\n\\npublic static void main(String args[])\\n{\\n Shell shell = new Shell();\\n shell.open();\\n shell.setText(\"draw2d Figures\");\\n\\n LightweightSystem lws = new LightweightSystem(shell);\\n\\n IFigure panel = new Figure();\\n panel.setLayoutManager( new FlowLayout(FlowLayout.HORIZONTAL) );\\n\\n lws.setContents(panel);\\n\\n Polyline polyline = new Polyline();\\n polyline.setStart(new Point( 5, 5));\\n polyline.addPoint(new Point( 5, 45));\\n polyline.addPoint(new Point( 45, 45));\\n polyline.addPoint(new Point( 45, 5));\\n panel.add(polyline);\\n\\n Polyline polyline2 = new Polyline();\\n polyline2.setStart(new Point( 5, 5));\\n polyline2.addPoint(new Point( 45, 45));\\n panel.add(polyline2);\\n\\n Display display = Display.getDefault();\\n while( !shell.isDisposed() )\\n {\\n  if( !display.readAndDispatch() )\\n   display.sleep();\\n }\\n\\n}',\n",
    " 'creation_date': '2002-05-31 09:17 EDT'}\n",
    "```\n",
    "**Remarques:**\n",
    "\n",
    "- La date de cr√©ation doit √™tre repr√©sent√©e sous la forme d'un \"ann√©e-mois-jour heure:minute fuseau horaire\". Si un bug report n'est pas un duplicata, alors dup_id doit √™tre None.\n",
    "- Indice: [lxml parse](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser) est plus rapide que html.parser*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "Implement ```extract_data_from_page function```. This function extracts the following information from the html: bug report id, creation date, title, product, component, the report id which it is duplicate and description.\n",
    "\n",
    "*extract_data_from_page* function returns a dictionary with the following structure:\n",
    "\n",
    "\n",
    " ```python\n",
    "{\"report_id\": int, \n",
    "  \"dup_id\": int or None (the report id which it is duplicate), \n",
    "  \"component\": string, \n",
    "  \"product\": string, \n",
    "  \"summary\": str, \n",
    "  \"description\": string, \n",
    "  \"creation_date\": int}\n",
    " ```\n",
    "For instance, when extract_data_from_page receives \"bug_report/18431.html\", it returns:\n",
    " \n",
    "```python\n",
    "{'report_id': 18431,\n",
    " 'dup_id': 27227,\n",
    " 'component': 'GEF-Legacy Draw2d',\n",
    " 'product': 'GEF',\n",
    " 'summary': 'Polylines ignored by FLowLayout',\n",
    " 'description': 'I tried a poor sample but it\\'s not working the way I thought it would be. Look \\nat the following source. I have a figure with FlowLayout and add 2 polylines to \\nit but the polylines aren\\'t arranged in FlowLayout. Both polylines are pushed \\ninto the first flow element. Any ideas why?\\n\\n\\n\\npublic static void main(String args[])\\n{\\n Shell shell = new Shell();\\n shell.open();\\n shell.setText(\"draw2d Figures\");\\n\\n LightweightSystem lws = new LightweightSystem(shell);\\n\\n IFigure panel = new Figure();\\n panel.setLayoutManager( new FlowLayout(FlowLayout.HORIZONTAL) );\\n\\n lws.setContents(panel);\\n\\n Polyline polyline = new Polyline();\\n polyline.setStart(new Point( 5, 5));\\n polyline.addPoint(new Point( 5, 45));\\n polyline.addPoint(new Point( 45, 45));\\n polyline.addPoint(new Point( 45, 5));\\n panel.add(polyline);\\n\\n Polyline polyline2 = new Polyline();\\n polyline2.setStart(new Point( 5, 5));\\n polyline2.addPoint(new Point( 45, 45));\\n panel.add(polyline2);\\n\\n Display display = Display.getDefault();\\n while( !shell.isDisposed() )\\n {\\n  if( !display.readAndDispatch() )\\n   display.sleep();\\n }\\n\\n}',\n",
    " 'creation_date': '2002-05-31 09:17 EDT'}\n",
    "```\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- Creation date have to be represented as \"year-mont-day hour:minute timezone\". If bug report is not duplicate, dup_id have to be None.\n",
    "- HINT: [lxml parse](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser) is faster than html.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Installing beautifulsoup4, if not already installed.\n",
    "!conda install --yes --prefix {sys.prefix} beautifulsoup4\n",
    "\n",
    "# Installing lxml parsing library to use instead of the built in parser (html.parser)\n",
    "!conda install --yes --prefix {sys.prefix} lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import dateutil.parser as dparser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\"\"\"\n",
    "    Scrap le contenu du rapport de html \n",
    "    \n",
    "    :param pagepath: le chemin du fichier html. \n",
    "    :return: \n",
    "        {\n",
    "        \"report_id\": int,\n",
    "        \"dup_id\": int or None (l'identifiant du rapport dont il est dupliqu√©), \n",
    "        \"component\": string, \n",
    "        \"product\": string, \n",
    "        \"summary\": string, \n",
    "        \"description\": string, \n",
    "        \"creation_date\": string\n",
    "        }\n",
    "\"\"\"\n",
    "def extract_data_from_page(pagepath):\n",
    "    file = open(pagepath, \"r\", encoding=\"utf8\")\n",
    "    page_content = file.read()\n",
    "    file.close()\n",
    "\n",
    "    content = {}\n",
    "    parsed_html = BeautifulSoup(page_content, \"lxml\")\n",
    "\n",
    "    content['report_id'] = int(os.path.basename(pagepath).replace('.html', ''))\n",
    "\n",
    "    print(os.path.basename(pagepath))\n",
    "    # Dup ID\n",
    "    parsed_anchors = parsed_html.find(id=\"static_bug_status\").select(\"span > a\")\n",
    "    if parsed_anchors == []:\n",
    "        content['dup_id'] = None\n",
    "    else:\n",
    "        extracted_id = re.findall(r'\\d+', parsed_anchors[0].get_text(strip=True))\n",
    "        content['dup_id'] = int(\"\".join(extracted_id))\n",
    "\n",
    "    content['component'] = re.findall(r'^\\w+', parsed_html.find(\"td\", {\"id\":\"field_container_component\"}).get_text(strip=True))[0]\n",
    "\n",
    "    content['product'] = parsed_html.find(\"td\", {\"id\":\"field_container_product\"}).get_text(strip=True)\n",
    "\n",
    "    content['summary'] = parsed_html.find(id=\"short_desc_nonedit_display\").get_text(strip=True)\n",
    "\n",
    "    content['description'] = parsed_html.find(id=\"c0\").select('pre')[0].get_text(strip=True)\n",
    "\n",
    "    parsed_creation_date = parsed_html.find(id=\"bz_show_bug_column_2\").select(\"td\")[0].get_text(strip=False)\n",
    "    content['creation_date']= re.findall(r'^\\d+\\-\\d+\\-\\d+\\ \\d+\\:\\d+\\ \\w\\w\\w', parsed_creation_date)[0]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Extraction de texte √† partir de HTML / Text extraction from HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "# Indexer chaque rapport par son identifiant \n",
    "index_path = os.path.join(FOLDER_PATH, 'bug_reports.json')\n",
    "\n",
    "if os.path.isfile(index_path):\n",
    "    report_index = json.load(open(index_path))\n",
    "else:\n",
    "    # Extraire le contenu d'une page Web \n",
    "\n",
    "    # Cela peut √™tre lent (environ 15 minutes). Testez votre code avec un petit √©chantillon. l'analyse lxml est plus rapide que html.parser\n",
    "    files = [os.path.join(PAGE_FOLDER, filename) for filename in os.listdir(PAGE_FOLDER)]\n",
    "    reports = [extract_data_from_page(f) for f in tqdm.tqdm(files)]\n",
    "    report_index = dict(((report['report_id'], report) for report in reports ))\n",
    "\n",
    "    # Save preprocessed bug reports\n",
    "    json.dump(report_index, open(index_path,'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Pr√©traitement des donn√©es / Data preprocessing\n",
    "\n",
    "Le pr√©traitement des donn√©es est une tache cruciale en fouille de donn√©es. Cette √©tape nettoie et transforme les donn√©es brutes dans un format qui permet leur analyse, et leur utilisation avec des algorithmes de *machine learning*. En traitement des langages (natural language processing, NLP), la *tokenization* et le *stemming* sont des √©tapes cruciales. De plus, vous impl√©menterez une √©tape suppl√©mentaire pour filtrer les mots sans importance.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task clean and transform the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 5.1 - Tokenization\n",
    "\n",
    "Cette √©tape permet de s√©parer un texte en s√©quence de *tokens* (= jetons, ici des mots, symboles ou ponctuation).\n",
    "\n",
    "Par exemple, la phrase \"It's the student's notebook.\" peut √™tre s√©par√© en liste de tokens de cette mani√®re: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
    "\n",
    "--- \n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). For instance, the sentence \"It's the student's notebook.\" can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "\n",
    "### 5.1.1 - Question 2 (1 point) \n",
    "\n",
    "**Implement** ```tokenize_space_punk``` **qui remplace la ponctuation par un espace, puis tokenise les jetons s√©par√©s par des espaces (espace, tabulation, nouvelle ligne). Tous les tokenizers doivent mettre les jetons en minuscule.**\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**Implement** ```tokenize_space_punk``` **that replaces the punctuation to space and then tokenizes the tokens that are separated by whitespace (space, tab, newline). You have to lowercase the tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "\"\"\"\n",
    "This tokenizer replaces punctuation to spaces and then tokenizes the tokens that are separated by whitespace (space, tab, newline).\n",
    "\"\"\"\n",
    "def tokenize_space_punk(text: str):\n",
    "    # return (\"\".join(word.strip(string.punctuation) for word in text)).split()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "tokenize_space_punk(\"\"\"Hello, this is a test sentence! ...\n",
    "I am sure that this tokenizer works great!\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Stop words removal\n",
    "\n",
    "### 5.2.1 - Question 3 (0.5 point)\n",
    "\n",
    "Certains tokens sont sans importance pour la comparaison, car ils apparaissent dans la majorit√© des discussions. Les supprimer r√©duit la dimension du vecteur et acc√©l√®re les calculs.\n",
    "\n",
    "**Expliquez quels tokens sont sans importances pour la comparaison des discussions. Impl√©mentez la fonction** ```filter_tokens``` **qui retire ces mots de la liste des tokens.**\n",
    "\n",
    "*Vous pouvez utiliser un ensemble de mots pr√©d√©fini.* \n",
    "\n",
    "---\n",
    "\n",
    "There are a set of tokens, call stop words, that are not signficant to the similarity comparison since they appear in most of bug report pages. Thus, removing them decreases the vector dimensionality and turns the similarity calculation computationally cheaper. \n",
    "\n",
    "**Describe the tokens that can be removed without affecting the similarity comparison. Moreover, implement the function** ```filter_tokens``` **that removes these words from a list of tokens.**\n",
    "\n",
    "*You can use a predefined set of words from a library.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Answer\n",
    "\n",
    "An example set of stop words is \"a\", \"the\", \"of\", \"is\", \"are\" and other more commonly used words in the english language (or any other language respectively). These words can be filtered out and the sentences extracted will still remain senseful, such that we can extract their meanings.\n",
    "\n",
    "There is a fairly popular list of predefined stop words available as a gist on GitHub, used by a lot of other developers when working on NLP projects that can be found [here](https://gist.github.com/sebleier/554280). Furthermore, the NLTK library also has a module that allows developers to quickly obtain a list of stopwords. This module is used in the implementation of the filter_tokens method right below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def filter_tokens(tokens: list[str]):\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token.lower() not in stopwords:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "filter_tokens(\"Example sentence used to showcase the filtering of stopwords\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - Stemming\n",
    "\n",
    "La racinisation (stemming) est un proc√©d√© de transformation des flexions en leur radical ou racine. Par example, en anglais, la racinisation de \"fishing\", \"fished\" and \"fish\" donne \"fish\" (stem). \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The process to convert tokens with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*. For instance, the word \"fishing\", \"fished\" , \"fish\" and \"fisher\" are reduced to the stem \"fish\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "word1 = ['I', 'tried', 'different', 'fishes']\n",
    "\n",
    "print([ stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'will', 'tries', 'only', 'one', 'fishing']\n",
    "print([ stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 - Question 4 (0.5 point) \n",
    "\n",
    "\n",
    "**Expliquez comment et pourquoi le stemming est utile √† notre syst√®me.**\n",
    "\n",
    "---\n",
    "\n",
    "**Explain how stemming can benift our system.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is an important part of the preprocessing of texts in NLP. It facilitates data management in big data systems by reducing costs in size while preventing much loss of information. This process can also be used to reduce redundancy and eliminate a lot of duplicates in a text corpus. \n",
    "\n",
    "Stemming, however, can produce unwanted results, because of the stemmer not being aware of the actual root word. For example, the word \"went\" should root to the word \"go\", however, this is not the case in a lot of stemming algorithms.\n",
    "\n",
    "Unfortunately, this process is very language dependant and thus cannot be used all the time. All in all, it is a useful and well thought of process, but must be performed carefully depending on the data that we are dealt, in order to maximize efficiency and prevent data loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Repr√©sentation des donn√©es / Data Representation\n",
    "\n",
    "\n",
    "# 6.1 - Bag of Words (BoW)\n",
    "\n",
    "De nombreux algorithmes demandent des entr√©es qui sont toutes de la m√™me taille. Cela n'est pas toujours le cas, notamment pour des donn√©es textuelles qui peuvent avoir un nombre variable de mots. \n",
    "\n",
    "Par exemple, consid√©rons la phrase 1, ‚ÄùBoard games are much better than video games‚Äù et la phrase 2, ‚ÄùMonopoly is an awesome game!‚Äù La table ci-dessous montre un exemple d'un moyen de repr√©sentation de ces deux phrases en utilisant une repr√©sentation fixe : \n",
    "\n",
    "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "\n",
    "Chaque colonne repr√©sente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurrence des mots dans une phrase. Ainsi, la valeur 2 √† la position (1,7) est due au mot *\"games\"* qui appara√Æt deux fois dans la phrase 1. \n",
    "\n",
    "Ainsi, chaque ligne √©tant de longueur 13, on peut les utiliser comme vecteur pour repr√©senter les phrases 1 et 2. Ainsi, c'est cette m√©thode que l'on appelle *Bag-of-Words* : c'est une repr√©sentation de documents par des vecteurs dont la dimension est √©gale √† la taille du vocabulaire, et qui est construite en comptant le nombre d'occurrences de chaque mot. Ainsi, chaque token est ici associ√© √† une dimension.\n",
    "\n",
    "*Pour plus de simplicit√©, nous consid√©rons le token et le mot comme interchangeables.**\n",
    "\n",
    "---\n",
    "\n",
    "Many algorithms only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two sentences: ‚ÄùBoard games are much better than video games‚Äù and ‚ÄùMonopoly is an awesome game!‚Äù. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "*For simplicity, we consider token and word as interchangeable.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - TF-IDF\n",
    "\n",
    "\n",
    "L'utilisation de la fr√©quence brute d'apparition des mots, comme c'est le cas avec bag-of-words, peut √™tre probl√©matique. En effet, peu de tokens auront une fr√©quence tr√®s √©lev√©e dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance √† biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas √† les discriminer. Par exemple, le mot \"*de*\" appara√Æt dans beaucoup de documents de la base de donn√©es, et pour autant, avoir ce mot en commun ne permet pas de conclure que des documents sont similaires. √Ä l‚Äôinverse, le mot \"*g√©nial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'√™tre positifs. TF-IDF est donc une m√©thode qui permet de pallier √† ce probl√®me.\n",
    "\n",
    "TF-IDF pond√®re le vecteur en utilisant une fr√©quence de document inverse (IDF) et une fr√©quence de termes (TF).\n",
    "\n",
    "TF est l'information locale sur l'importance qu'a un mot dans un document donn√©, tandis que IDF mesure la capacit√© de discrimination des mots dans un jeu de donn√©es. \n",
    "\n",
    "L'IDF d'un mot se calcule de la fa√ßon suivante:\n",
    "\\begin{equation}\n",
    "\t\\operatorname{IDF}(t) = \\ln\\left( \\frac{N+1}{\\operatorname{df}(t)+1} \\right) + 1,\n",
    "\\end{equation}\n",
    "o√π $t$ est un token, $N$ est le nombre de documents dans l'ensemble de donn√©es, et $\\operatorname{df}(\\cdot)$  est le nombre de documents qui contiennent un mot $i$.\n",
    "\n",
    "Le nouveau poids d'un mot $t$ dans un texte peut ensuite √™tre calcul√© de la fa√ßon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw(t) = \\operatorname{tf}(t) \\times \\operatorname{IDF}(t),\n",
    "\\end{equation}\n",
    "o√π $\\operatorname{tf}(\\cdot)$ est le terme fr√©quence du mot ùëñ dans le document ùëó, c'est-√†-dire le nombre de fois qu'un mot appara√Æt dans le document. *Nous appelons repr√©sentation TF-IDF lorsque les poids de la repr√©sentation BoW sont calcul√©s au moyen de TF-IDF.*\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "part of reports. Thus, having the word *of* does not make\n",
    "reports more or less similar. However, the word *terrible* is rarer and reports that\n",
    "have this word are more likely to be negative. TF-IDF is a technique\n",
    "that overcomes the word frequency disadvantages.\n",
    "\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\t\\operatorname{IDF}(t) = \\ln\\left( \\frac{N+1}{\\operatorname{df}(t)+1} \\right) + 1,\n",
    "\\end{equation}\n",
    "where $t$ is a token, $N$ is the number of documents in the dataset, and $\\operatorname{df}(\\cdot)$ is the number of documents that contain a word $i$.\n",
    "The new weight of a word $t$ in a text is defined as:\n",
    "\\begin{equation}\n",
    "\tw(t) = \\operatorname{tf}(t) \\times \\operatorname{IDF}(t),\n",
    "\\end{equation}\n",
    "where $\\operatorname{tf}(\\cdot)$ is the term frequency of word $i$ , i.e, number of times that a word appears in the document. *We call TF-IDF representation when the weights of the BoW represention are computed by means of TF-IDF.*\n",
    "\n",
    "\n",
    "### 6.2.1 - Question 5 (3.5 points)\n",
    "\n",
    "**Maintenant, vous devez impl√©menter TF-IDF. La m√©thode** ```fit``` **calcule IDF sur la base des donn√©es textuelles dans X et la m√©thode** ```transform```  **transforme un texte en une repr√©sentation TF-IDF.**\n",
    "\n",
    "*Remarques*:\n",
    "\n",
    "- Attention √† ce que ```transform``` puisse recevoir des tokens qui n'√©taient pas dans X. Dans ce cas, consid√©rez que $\\operatorname{df}(\\cdot)=0$.\n",
    "- Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn sauf numpy.\n",
    "- Dans ce TP, nous repr√©sentons chaque texte comme une liste de tuples dans laquelle le tuple est compos√© du roken et de son poids calcul√© par TF-IDF. Pour am√©liorer l'efficacit√©, nous trions les tuples par token. Ainsi, par exemple, la phrase 1 du tableau pr√©c√©dent est repr√©sent√©e comme suit : ```[ (\"are\", 1.4054), (\"better\", 1.4054), (\"board\", 1.4054), (\"games\", 2.8108), (\"much\", 1.4054), (\"than\", 1.4054), (\"video\", 1.4054)]```.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Now, you have to implement TF-IDF. Method** ```fit``` **computes IDF based on the textual data in X and method** ```transform```  **transforms a text to a TF-IDF representation.**\n",
    "\n",
    "*Notes*:\n",
    "\n",
    "- Be careful that ```transform```can receive tokens that were not in X. In this case, consider that $\\operatorname{df}(\\cdot)=0$.\n",
    "- For this question, except numpy, you cannot use any external python library (e.g., scikit-learn).\n",
    "- In this TP, we represent each text as list of tuples in which the tuple are composed of the token and its weight computed by TF-IDF. To improve efficiency, we sort the tuples by the tokens. Thus, for instance, Sentence 1 of previous table is represented as: ```[ (\"are\", 1.4054), (\"better\", 1.4054), (\"board\", 1.4054), (\"games\", 2.8108), (\"much\", 1.4054), (\"than\", 1.4054), (\"video\", 1.4054)]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import chain\n",
    "\n",
    "class TFIDF:\n",
    "    \"\"\"\n",
    "    Apprendre les valeurs IDF bas√©es sur les donn√©es textuelles dans X\n",
    "\n",
    "    X:  liste des listes de tokens. Par example, X=[['video','awesome', 'The'], ['house', 'The']] \n",
    "\n",
    "    Computes the IDF Based on the following formula: IDF(t) = ln((N + 1)/(df(t) + 1))\n",
    "    Where: \n",
    "        - t is a given word\n",
    "        - N is the amount of documents in the text corpus\n",
    "        - df(t) is the amount of documents that contain the word t\n",
    "    \n",
    "    Also provides us with tf: the term frequency\n",
    "        The term frequency is the amount of times that the word comes up in its list of tokens.\n",
    "\n",
    "        DF: How many times the word comes up in all of the combined documents\n",
    "        TF: How many times the words comes up in the original document.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, corpus: list[list[str]]):\n",
    "        # First we make sure the entirety of the words are lowercased\n",
    "        corpus = list(map(lambda vector: [word.lower() for word in vector], corpus))\n",
    "\n",
    "        # We can then flatten the corpus to extract the list of unique words. \n",
    "        flattened_corpus = list(chain.from_iterable(corpus))\n",
    "        self.words = set(flattened_corpus)\n",
    "\n",
    "        # The flattened corpus is also used to count the DF for each word:\n",
    "        dfs = { word: 0 for word in flattened_corpus }\n",
    "        for word in dfs:\n",
    "            for vector in corpus:\n",
    "                if word in vector:\n",
    "                    dfs[word] += 1\n",
    "\n",
    "        # We can finally compute the IDF for each word in the unique set of words:\n",
    "        self.idfs = { word: 1 + math.log((len(corpus) + 1)/(dfs[word] + 1)) for word in self.words }\n",
    "    \"\"\" \n",
    "    \n",
    "    Transforme un texte en une repr√©sentation TF-IDF\n",
    "    \n",
    "    tokens: liste de tokens.Par example, tokens=['video','awesome', 'The', 'The']\n",
    "    \n",
    "    :return: list of lists of tuples. Renvoie la repr√©sentation TF-IDF des textes.  \n",
    "            Par example, [('video', 1.4054), ('awesome', 1.4054), ('The', 2.0)]\n",
    "    \"\"\"\n",
    "    def transform(self, tokens):\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "\n",
    "        # The term frequency is computed by counting the number of occurences of each word inside each document in the corpus:\n",
    "        tfs_list = { token: tokens.count(token) for token in tokens }\n",
    "\n",
    "        tfidfs = []\n",
    "\n",
    "        for word, idf in self.idfs.items():\n",
    "            if word in tfs_list:\n",
    "                tfidfs.append((word, idf * tfs_list[word]))\n",
    "\n",
    "        return tfidfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 - Word embedding\n",
    "\n",
    "R√©cemment, un nouveau type de repr√©sentation, appel√© word embedding ou word vector, s'est r√©v√©l√© tr√®s utile pour la PNL. Dans les plongements de mots, les mots sont repr√©sent√©s comme des vecteurs r√©els, de faible dimension et denses. Ces vecteurs d√©crivent les positions des mots dans un nouvel espace de caract√©ristiques qui conservent les informations syntaxiques et s√©mantiques. Contrairement √† d'autres repr√©sentations, word embeddings  souffrent moins de la mal√©diction de la dimensionnalit√© et am√©liorent la capacit√© du mod√®le √† g√©rer les mots inconnus et rares dans la formation. Par ailleurs,\n",
    "en utilisant word embedding, il est possible d'effectuer des op√©rations arithm√©tiques et\n",
    "calculer la distance entre les mots. \n",
    "\n",
    "---\n",
    "\n",
    "Recently, a new type of representation, called word embeddings, word vectors, or distributed word representation, has been shown very useful for NLP. In word embeddings, words are represented as real, low dimensional and dense vectors. Those vectors describe word positions in a new feature space that retain syntactic and semantic information. In contrast to other representations, word embeddings suffer less with the curse of dimensionality and improve\n",
    "the model capability to handle unknown and rare words in the training. Furthermore,\n",
    "using distributed word representations, it is possible to perform arithmetical operations and\n",
    "calculate the distance between words. \n",
    " \n",
    "\n",
    "### 6.3.1 - Question 6 (3 points)\n",
    "\n",
    "Dans ce TP, nous utiliserons des incorporations de mots pour g√©n√©rer une repr√©sentation dense du texte, appel√©e *text embedding*.\n",
    "Dans ce contexte, le texte peut contenir une phrase ou plusieurs paragraphes.\n",
    "Le text embedding est calcul√© comme la moyenne des vecteurs des mots :\n",
    "\\begin{equation}\n",
    "\te_s = \\frac{1}{|s|} \\sum_{t \\in s} w_t,\n",
    "\\end{equation}\n",
    "o√π $|s|$ est la longueur du texte $s$, $w_t$ est word embedding du token $t$ dans $s$ et $e_s$ est text embedding de $s$.\n",
    "\n",
    "Vous utiliserez les vector des mots pr√©-entra√Æn√©es de *glove.42B.300d_clear.txt* dans le dossier *dataset*.\n",
    "Dans chaque ligne de ce fichier texte, il y a les tokens et leurs valeurs vectorielles. Les valeurs et les tokens sont s√©par√©s par des espaces. Dans ce fichier, la longueur de word embedding est de 300.\n",
    "\n",
    "\n",
    "**Maintenant, vous devez impl√©menter la m√©thode** ```generate_embedding``` **qui g√©n√®re text embedding. L'attribut token2vec est un dictionnaire qui relie un mot √† son vecteur.**\n",
    "\n",
    "\n",
    "*Remarques:*\n",
    "\n",
    "- Ne pas tenir compte des mots qui n'existent pas dans glove.42B.300d_clear.txt\n",
    "- Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn sauf numpy.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this TP, we will use word embeddings to generate a dense representation of text, called text embedding.\n",
    "In this context, text could contain a sentence or multiple paragraphs.\n",
    "The text embedding is computed as average of the vectors of the words:\n",
    "\\begin{equation}\n",
    "\te_s = \\frac{1}{|s|} \\sum_{t \\in s} w_t,\n",
    "\\end{equation}\n",
    "where $|s|$ is the length of text $s$, $w_t$ is the word embedding of token $t$ in $s$, and $e_s$ is the embedding of $s$.\n",
    "\n",
    "You will use the pre-trained word embeddings from *glove.42B.300d_clear.txt* in the folder *dataset*.\n",
    "In each line of this text file, there are the tokens and theirs vector values. Values and tokens are separated by spaces. In this file, the word embedding length is 300.\n",
    "\n",
    "****Now, you have to implement the method** ```generate_embedding``` **that generates the text embedding. The attribute token2vec is a dictionary that links a word to its vector.****\n",
    "\n",
    "*Notes:*\n",
    "- Disconsider words that does not exist in glove.42B.300d_clear.txt\n",
    "- For this question, except for numpy, you cannot use any external python library (e.g., scikit-learn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TextEmbedding:\n",
    "    \n",
    "    def __init__(self):\n",
    "        with open(word_vec_path, 'r', encoding=\"utf8\") as word_vec_file:\n",
    "            lines = word_vec_file.readlines()\n",
    "        \n",
    "        self.token2vec = dict()\n",
    "        for line in lines:\n",
    "            inputs = line.split()\n",
    "            self.token2vec.update({ inputs[0]: list(map(float, inputs[1:])) })\n",
    "\n",
    "    \"\"\"\n",
    "    G√©n√©rez text embedding en tant que moyenne de word embeddings dans tokens. \n",
    "    Ne pas tenir compte des mots qui n'existent pas dans glove.42B.300d_clear.txt\n",
    "    tokens: liste de tokens.Par example, [\"are\", \"better\", \"board\", \"much\"]\n",
    "    :return:  text embedding (numpy ou liste de nombres r√©els)\n",
    "    \"\"\"\n",
    "    def generate_embedding(self, tokens: list[str]):\n",
    "        WORD_EMBEDDING_LENGTH = 300\n",
    "        embeddings = [0 for i in range(WORD_EMBEDDING_LENGTH)]\n",
    "\n",
    "        s = len(tokens)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token not in self.token2vec:\n",
    "                s -= 1\n",
    "\n",
    "        if s == 0:\n",
    "            return embeddings\n",
    "\n",
    "        for index, _ in enumerate(embeddings):\n",
    "            for token in tokens:\n",
    "                if token in self.token2vec:\n",
    "                    embeddings[index] += self.token2vec[token][index]\n",
    "\n",
    "            embeddings[index] /= s\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Pipeline\n",
    "\n",
    "La *pipeline* est la s√©quence d'√©tapes de pr√©traitement des donn√©es qui transforme les donn√©es brutes dans un format qui permet leur analyse.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The pipeline is a sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. \n",
    "\n",
    "\n",
    "\n",
    "## 7.1 - Question 7 (2 points) \n",
    "\n",
    "**Maintenant, vous devez impl√©menter le pipeline en suivant les instructions dans les cellules ci-dessous.**\n",
    "\n",
    "---\n",
    "\n",
    "**Now, you have to implement the pipeline following the instructions in the cells below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Apr√®s l'ex√©cution du pipeline, chaque rapport dans report_index doit contenir quatre cl√©s :\n",
    "     - summary_tfidf : repr√©sentation TF-IDF du r√©sum√©\n",
    "     - desc_tfidf : repr√©sentation TF-IDF de la description\n",
    "     - summary_vec : le text embedding du r√©sum√©\n",
    "     - desc_vec : le text embedding  de la description\n",
    "\n",
    "Par exemple, le rapport 18431 dans report_index (report_index[18431]) est : \n",
    "\n",
    "{'report_id': 18431,\n",
    " 'dup_id': 27227,\n",
    " 'component': 'GEF-Legacy Draw2d',\n",
    " 'product': 'GEF',\n",
    " 'summary': 'Polylines ignored by FLowLayout',\n",
    " 'description': 'I tried a poor sample but it\\'s not working the way I thought it would be. Look \\nat the following source. I have a figure with FlowLayout and add 2 polylines to \\nit but the polylines aren\\'t arranged in FlowLayout. Both polylines are pushed \\ninto the first flow element. Any ideas why?\\n\\n\\n\\npublic static void main(String args[])\\n{\\n Shell shell = new Shell();\\n shell.open();\\n shell.setText(\"draw2d Figures\");\\n\\n LightweightSystem lws = new LightweightSystem(shell);\\n\\n IFigure panel = new Figure();\\n panel.setLayoutManager( new FlowLayout(FlowLayout.HORIZONTAL) );\\n\\n lws.setContents(panel);\\n\\n Polyline polyline = new Polyline();\\n polyline.setStart(new Point( 5, 5));\\n polyline.addPoint(new Point( 5, 45));\\n polyline.addPoint(new Point( 45, 45));\\n polyline.addPoint(new Point( 45, 5));\\n panel.add(polyline);\\n\\n Polyline polyline2 = new Polyline();\\n polyline2.setStart(new Point( 5, 5));\\n polyline2.addPoint(new Point( 45, 45));\\n panel.add(polyline2);\\n\\n Display display = Display.getDefault();\\n while( !shell.isDisposed() )\\n {\\n  if( !display.readAndDispatch() )\\n   display.sleep();\\n }\\n\\n}',\n",
    " 'creation_date': '2002-05-31 09:17 EDT',\n",
    " 'summary_vec': array([x.xxxxx, x.xxxxx, ... ]),\n",
    " 'desc_vec':  array([x.xxxxx, x.xxxxx, ... ]),\n",
    " 'summary_tfidf': [['flowlayout', x], ['ignor', x], ['polylin', x]],\n",
    " 'desc_tfidf': [['2', x], ['45', x], ['5', x], ['add', x], ['addpoint', x], ... ]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "√âtape 1. Tokeniser et supprimer les mots vides dans le r√©sum√© et la description de chaque rapport.\n",
    "\"\"\"\n",
    "#TODO: mettre en ≈ìuvre l'√©tape 1 \n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\"\"\"\n",
    "After the pipeline execution, each report in report_index should contain four keys:\n",
    "    - summary_tfidf: TF-IDF representation of the summary\n",
    "    - desc_tfidf: TF-IDF representation of the descripton\n",
    "    - summary_vec: the text embedding of the summary\n",
    "    - desc_vec: the text embedding of the description\n",
    "\n",
    "For example, report 18431 in report_index (report_index[18431]) is:\n",
    "\n",
    "{'report_id': 18431,\n",
    " 'dup_id': 27227,\n",
    " 'component': 'GEF-Legacy Draw2d',\n",
    " 'product': 'GEF',\n",
    " 'summary': 'Polylines ignored by FLowLayout',\n",
    " 'description': 'I tried a poor sample but it\\'s not working the way I thought it would be. Look \\nat the following source. I have a figure with FlowLayout and add 2 polylines to \\nit but the polylines aren\\'t arranged in FlowLayout. Both polylines are pushed \\ninto the first flow element. Any ideas why?\\n\\n\\n\\npublic static void main(String args[])\\n{\\n Shell shell = new Shell();\\n shell.open();\\n shell.setText(\"draw2d Figures\");\\n\\n LightweightSystem lws = new LightweightSystem(shell);\\n\\n IFigure panel = new Figure();\\n panel.setLayoutManager( new FlowLayout(FlowLayout.HORIZONTAL) );\\n\\n lws.setContents(panel);\\n\\n Polyline polyline = new Polyline();\\n polyline.setStart(new Point( 5, 5));\\n polyline.addPoint(new Point( 5, 45));\\n polyline.addPoint(new Point( 45, 45));\\n polyline.addPoint(new Point( 45, 5));\\n panel.add(polyline);\\n\\n Polyline polyline2 = new Polyline();\\n polyline2.setStart(new Point( 5, 5));\\n polyline2.addPoint(new Point( 45, 45));\\n panel.add(polyline2);\\n\\n Display display = Display.getDefault();\\n while( !shell.isDisposed() )\\n {\\n  if( !display.readAndDispatch() )\\n   display.sleep();\\n }\\n\\n}',\n",
    " 'creation_date': '2002-05-31 09:17 EDT',\n",
    " 'summary_vec': array([x.xxxxx, x.xxxxx, ... ]),\n",
    " 'desc_vec':  array([x.xxxxx, x.xxxxx, ... ]),\n",
    " 'summary_tfidf': [['flowlayout', x], ['ignor', x], ['polylin', x]],\n",
    " 'desc_tfidf': [['2', x], ['45', x], ['5', x], ['add', x], ['addpoint', x], ... ]}\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Step 1. Tokenize and remove the stop words in the summary and description of each report.\n",
    "\"\"\"\n",
    "#TODO: implement step 1\n",
    "report_index = json.load(open(index_path))\n",
    "report_index = { int(report_id): data for report_id, data in report_index.items() }\n",
    "\n",
    "for key in report_index:\n",
    "    report_index[key]['summary'] = filter_tokens(\n",
    "        tokenize_space_punk(report_index[key]['summary'])\n",
    "    )\n",
    "\n",
    "    report_index[key]['description'] = filter_tokens(\n",
    "        tokenize_space_punk(report_index[key]['description'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "√âtape 2 . G√©n√©rez les text embedding du r√©sum√© et de la description pour chaque rapport.\n",
    "Utilisez les textes pr√©trait√©s √† l'√©tape 1 pour g√©n√©rer les embeddings.\n",
    "\"\"\"\n",
    "\n",
    "#TODO: mettre en ≈ìuvre l'√©tape 2 \n",
    "pass\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Step 2. Generate the embedding of the summary and description for each report. \n",
    "Use the texts preprocessed by step 1 to generate the embeddings.\n",
    "\"\"\"\n",
    "\n",
    "#TODO: implement step 2\n",
    "\n",
    "te = TextEmbedding()\n",
    "\n",
    "for key in report_index:\n",
    "    report_index[key]['summary_vec'] = te.generate_embedding(report_index[key]['summary'])\n",
    "    report_index[key]['desc_vec'] = te.generate_embedding(report_index[key]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "√âtape 3. Appliquez le stemming aux jetons g√©n√©r√©s √† partir de l'√©tape 1.\n",
    "\"\"\"\n",
    "#TODO : mettre en ≈ìuvre l'√©tape 3\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Step 3. Apply stemming to the tokens generated from Step 1. \n",
    "\"\"\"\n",
    "#TODO: implement step 3\n",
    "\n",
    "for key in report_index:\n",
    "    report_index[key]['summary'] = [stemmer.stem(word) for word in report_index[key]['summary']]\n",
    "    report_index[key]['description'] = [stemmer.stem(word) for word in report_index[key]['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "√âtape 4. Apprenez l'IDF en utilisant le r√©sum√© et la description de tous les rapports dans le ensemble d'entra√Ænement.\n",
    "Vous devez concat√©ner le contenu de ces deux champs et un rapport sera consid√©r√© comme un document afin de calculer la fr√©quence des documents.\n",
    "L'entr√©e de cette √©tape doit √™tre la sortie de l'√©tape 3 (stemmed tokens).\n",
    "\"\"\"\n",
    "# training_reports_set contient tous les rapports de l'ensemble d'entra√Ænement.\n",
    "# TODO: mettre en ≈ìuvre l'√©tape 4\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "\"\"\"\n",
    "Step 4. Learn the IDF using the summary and description of all the repors in the training set.\n",
    "You should concatenate the content of these two fields and a report will be considered a document in order to computer the document frequency.\n",
    "The input of this step should be output of Step 3 (stemmed tokens).\n",
    "\"\"\"\n",
    "\n",
    "# training_reports_set contains all reports in the training set.\n",
    "corpus = [report_index[str(report)]['summary'] + report_index[str(report)]['description'] for report in training_reports_set]\n",
    "\n",
    "# TODO: implement step 4\n",
    "tfidf = TFIDF()\n",
    "tfidf.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "√âtape 5. G√©n√©rez la repr√©sentation TF-IDF du r√©sum√© et de la description pour chaque rapport.\n",
    "L'entr√©e de cette √©tape doit √™tre la sortie de l'√©tape 3 (stemmed tokens).\n",
    "\"\"\"\n",
    "# A FAIRE : mettre en ≈ìuvre l'√©tape 5\n",
    "\n",
    "\"\"\"\n",
    "Step 5. Generate the TF-IDF representation of the summary and description for each report.\n",
    "The input of this step should be output of Step 3 (stemmed tokens).\n",
    "\"\"\"\n",
    "# TODO: implement step 5\n",
    "for key in report_index:\n",
    "    report_index[key]['summary_tfidf'] = tfidf.transform(report_index[key]['summary'])\n",
    "    report_index[key]['desc_tfidf'] = tfidf.transform(report_index[key]['description'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Cosine Similarity\n",
    "\n",
    "En PNL, la similarit√© cosinus est une fonction de similarit√© populaire utilis√©e pour comparer les vecteurs de documents. Cette fonction mesure √† quel point la direction de deux vecteurs est diff√©rente et ses valeurs sont comprises entre -1 et 1.\n",
    "\n",
    "La similarit√© en cosinus est d√©finie comme:\n",
    "\\begin{equation}\n",
    "    \\operatorname{cos(v, v')} = \\frac{\\sum_{i=1}^{n} v_i  v_i'}{\\sqrt{\\sum_{i=1}^{n} v_i^2} \\sqrt{\\sum_{i=1}^{n}v_i'^2}},\n",
    "\\end{equation}\n",
    "where $v$ and $v'$ sont des vecteurs de $n$ dimensions.\n",
    "\n",
    "\n",
    "-------------\n",
    "\n",
    "In NLP, the cosine similarity is a popular similarity function used to compare the vectors\n",
    "of documents. This function measures how different is the direction of two vectors, and its\n",
    "values are between -1 and 1. \n",
    "\n",
    "Cosine similarity is defined as:\n",
    "\\begin{equation}\n",
    "    \\operatorname{cos(v, v')} = \\frac{\\sum_{i=1}^{n} v_i  v_i'}{\\sqrt{\\sum_{i=1}^{n} v_i^2} \\sqrt{\\sum_{i=1}^{n}v_i'^2}},\n",
    "\\end{equation}\n",
    "where $v$ and $v'$ are vectors with $n$ dimensions.\n",
    "\n",
    "\n",
    "## 8.1 - Question 8 (2 points)\n",
    "\n",
    "**Vous devez impl√©menter deux fonctions :**\n",
    "\n",
    "  1. ```cosine_sim_tf_idf``` : calcule la similarit√© cosinus de deux repr√©sentations g√©n√©r√©es au moyen de TF-IDF.\n",
    "  2. ```cosine_sim_embedding``` : calcule la similarit√© de cosinus de deux text embeddings.\n",
    "\n",
    "\n",
    "*Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn sauf numpy. De plus, si le d√©nominateur est z√©ro, alors vous devez consid√©rer que cos(.) = 0*\n",
    "\n",
    "--------------------\n",
    "\n",
    "**You have to implement two functions:**\n",
    "\n",
    " 1. ```cosine_sim_tf_idf```: computes the cosine similarity of two representations generated using TF-IDF.\n",
    " 2. ```cosine_sim_embedding```: computes the cosine similarity of two field embeddings.\n",
    "\n",
    "\n",
    "*For this question, except for numpy, you cannot use any external python library (e.g., scikit-learn). Also, if the denominator is zero, so you should consider that cos(.) = 0*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import inf\n",
    "from itertools import chain\n",
    "\n",
    "def cosine_sim_tf_idf(r1: list[tuple[str, int]], r2: list[tuple[str, int]]) -> float:\n",
    "    \"\"\"\n",
    "    r1: Repr√©sentation TF-IDF . \n",
    "    r2: Repr√©sentation TF-IDF .\n",
    "    :return la similitude cosinus de r1 et r2\n",
    "    \"\"\"\n",
    "\n",
    "    # Assuming R1 and R2 have the same length? \n",
    "\n",
    "    def square_sum(r: dict):\n",
    "        return math.sqrt(sum(list(map(lambda key: pow(r[key], 2), r))))\n",
    "\n",
    "    numerator = 0\n",
    "    denom = 0\n",
    "    \n",
    "    r1_dict = dict(r1)\n",
    "    r2_dict = dict(r2)\n",
    "\n",
    "    numerator = sum([r1_dict[key]*r2_dict[key] if key in r2_dict else 0 for key in r1_dict])\n",
    "    denom = square_sum(r1_dict) * square_sum(r2_dict)\n",
    "\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "\n",
    "    return numerator/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_embedding(vec1, vec2):\n",
    "    \"\"\"\n",
    "    v1: text embedding\n",
    "    v2: text embedding\n",
    "   \n",
    "    :return la similitude cosinus de vec1 and vec2\n",
    "    \"\"\"\n",
    "    def square_sum(v: list):\n",
    "        return math.sqrt(sum(list(map(lambda x: pow(x, 2), v))))\n",
    "\n",
    "    numerator = sum([x * y for x, y in zip(vec1, vec2)])\n",
    "    denom = square_sum(vec1) * square_sum(vec2)\n",
    "\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "\n",
    "    return numerator/denom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Extraction de features / Feature extraction\n",
    "\n",
    "Nous formons un mod√®le de r√©gression logistique pour pr√©dire si une paire de rapports est dupliqu√© ou non. Les features utilis√©es pour la classification sont √©num√©r√©es ci-dessous :\n",
    "\n",
    "1. Similitude en cosinus de la repr√©sentation TF-IDF du r√©sum√©.\n",
    "1. Similitude en cosinus de la repr√©sentation TF-IDF de la description.\n",
    "1. Similitude en cosinus de les embedding de r√©sum√©s.\n",
    "1. Similitude en cosinus de les embedding de description.\n",
    "1. Un feature binaire qui est 1.0 lorsque le rapport provient des m√™mes composants. Sinon, c'est 0.0.\n",
    "1. Une feature binaire qui est 1.0 lorsque le rapport provient des m√™mes produits. Sinon, c'est 0.0.\n",
    "\n",
    "\n",
    "-------------\n",
    "\n",
    "We train a logistic regression model to predict whether a pair of reports is duplicate or not. The features employed for the classifion are listed below:\n",
    "\n",
    "1. Cosine similarity of the TF-IDF representation of summary.\n",
    "2. Cosine similarity of the TF-IDF representation of description.\n",
    "2. Cosine similarity of the summary embeddings.\n",
    "2. Cosine similarity of the description embeddings.\n",
    "2. A binary feature that is 1.0 when the report are from the same components. Othewise, it is 0.0.\n",
    "2. A binary feature that is 1.0 when the report are from the same products. Othewise, it is 0.0.\n",
    "\n",
    "\n",
    "\n",
    "## 9.1 - Question 9 (1 point)\n",
    "\n",
    "**Maintenant, impl√©mentez** ```extract_features``` **qui extrait les fonctionnalit√©s ci-dessus √† partir d'une paire de rapports.**\n",
    "\n",
    "*rm_ftr_idx* est un param√®tre qui permet de supprimer une ou plusieurs des features. Si *rm_ftr_idxs=[]*, alors toutes les features seront utilis√©es. Sinon, *rm_ftr_idxs* correspond aux positions des entit√©s dans la liste pr√©c√©dente √† supprimer. \n",
    "\n",
    "\n",
    "-------------\n",
    "\n",
    "\n",
    "**Now, implement** ```extract_features```**that extracts the above features from a pair of reports.** \n",
    "\n",
    "*rm_ftr_idx* is parameter that allow to remove one or more of the original features. If *rm_ftr_idxs=[]*, so all features will be used. Otherwise, *rm_ftr_idxs* is the positions of the features in the previous list to be removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(r1, r2, rm_ftr_idxs=[]):\n",
    "    \"\"\"\n",
    "    Extraire les features d'une paire (r1, r2).\n",
    "\n",
    "    La repr√©sentation TF-IDF du r√©sum√© et des descriptions est accessible sur r1 et r2 √† l'aide des cl√©s 'summary_tfidf' et 'desc_tfidf'.\n",
    "    Les embeddings du r√©sum√© et de la description est accessible sur r1 et r2 √† l'aide des cl√©s 'summary_vec' et 'desc_vec'. \n",
    "    \n",
    "    r1: Dictionnaire qui contient toutes les informations sur un rapport\n",
    "    r2: Dictionnaire qui contient toutes les informations sur un rapport\n",
    "    rm_ftr_idxs: Supprimer des features.\n",
    "         Par exemple, ftr_opt=[1,4] supprime la similarit√© en cosinus de la repr√©sentation TF-IDF du r√©sum√© et la similarit√© en cosinus des description embeddings.\n",
    "    \n",
    "    :return un vecteur de nombres r√©els.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    if 1 not in rm_ftr_idxs:\n",
    "     result.append(cosine_sim_tf_idf(r1['summary_tfidf'], r2['summary_tfidf']))\n",
    "\n",
    "    if 2 not in rm_ftr_idxs:\n",
    "     result.append(cosine_sim_tf_idf(r1['desc_tfidf'], r2['desc_tfidf']))\n",
    "\n",
    "    if 3 not in rm_ftr_idxs:\n",
    "     result.append(cosine_sim_embedding(r1['summary_vec'], r2['summary_vec']))\n",
    "\n",
    "    if 4 not in rm_ftr_idxs:\n",
    "     result.append(cosine_sim_embedding(r1['desc_vec'], r2['desc_vec']))\n",
    "\n",
    "    if 5 not in rm_ftr_idxs:\n",
    "     result.append(1 if r1['component'] == r2['component'] else 0)\n",
    "\n",
    "    if 6 not in rm_ftr_idxs:\n",
    "     result.append(1 if r1['product'] == r2['product'] else 0)\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "extract_features(report_index[100002], report_index[25], [1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Entra√Ænement/Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Charger des √©tiquettes √† partir de l'ensemble d'apprentissage\n",
    "y_train = np.asarray([ y for _, _, y in  training_pairs ])\n",
    "\n",
    "def train_clf(rm_ftr_idxs=[]):\n",
    "    # Extraire les features \n",
    "    X_train = np.asarray([ extract_features(report_index[r1], report_index[r2], rm_ftr_idxs) for r1, r2, _ in  training_pairs ])\n",
    "    return LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Question 10  (2.5 points)\n",
    "\n",
    "Maintenant, il est temps d'√©valuer le classificateur. En plus de cela, vous devrez effectuer une ablation study pour mesurer l'efficacit√© de chaque fonctionnalit√©. La ablation study consiste √† retirer un seul composant de l'architecture d'origine, et √† mesurer √† quel point cette modification isol√©e impacte les performances du mod√®le. Plus un composant affecte les performances, plus il est consid√©r√© comme efficace.\n",
    "\n",
    "Pour cette question, vous devez indiquer la pr√©cision du classificateur (fonction ```compute_acc```) pour les configurations suivantes :\n",
    "\n",
    "1. Classificateur avec toutes les features\n",
    "2. Classificateur sans *similitude en cosinus de la repr√©sentation TF-IDF du r√©sum√© et de la description*.\n",
    "4. Classificateur sans *similitude en cosinus des embeddings de r√©sum√© et de description*.\n",
    "5. Classificateur sans *comparaison de composants*.\n",
    "5. Classificateur sans *comparaison de produits*.\n",
    "    \n",
    "*D√©crivez √©galement les r√©sultats et les conclusions.*\n",
    "\n",
    "**Cette note de question sera p√©nalis√©e de 1.5 point lorsque la pr√©cision du classificateur avec toutes les features (6 features) est inf√©rieure √† 0,91.** \n",
    "\n",
    "-----\n",
    "\n",
    "Now, it is time to evaluate the classifier. Besides that, you will have to perform an ablation study to measure the effectiveness of each feature.  Ablation study consist in removing a single component from the original architecture, and measuring how much this isolated modification impacts the model performance. The more a component affects the performance, the more effective it is considered.\n",
    "\n",
    "For this question, you have to report the classifier accuracy (function ```compute_acc```) for the following configurations:\n",
    "\n",
    "1. Classifier with all features\n",
    "2. Classifier without *cosine similarity of the TF-IDF representation of summary and description*.\n",
    "4. Classifier without *cosine similarity of the summary and description embeddings*.\n",
    "5. Classifier without *component comparison*.\n",
    "5. Classifier without *product comparison*.\n",
    "    \n",
    "*Also, describe the results and findings.*\n",
    "\n",
    "\n",
    "**The mark will be penalized by 1.5 point when the accuracy of the classifier with full features (6 features) is lower than 0.91.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_acc(classifier, X):\n",
    "    # Compute accuracy\n",
    "    y = np.asarray([ y for _, _, y in  validation_pairs ])\n",
    "    return classifier.score(X, y)\n",
    "\n",
    "rm_arrays = [[], [1, 2], [3, 4], [5], [6]]\n",
    "print(compute_acc(train_clf(rm_arrays[0]), [extract_features(report_index[r1], report_index[r2], rm_arrays[0]) for r1, r2, _ in validation_pairs]))\n",
    "print(compute_acc(train_clf(rm_arrays[1]), [extract_features(report_index[r1], report_index[r2], rm_arrays[1]) for r1, r2, _ in validation_pairs]))\n",
    "print(compute_acc(train_clf(rm_arrays[2]), [extract_features(report_index[r1], report_index[r2], rm_arrays[2]) for r1, r2, _ in validation_pairs]))\n",
    "print(compute_acc(train_clf(rm_arrays[3]), [extract_features(report_index[r1], report_index[r2], rm_arrays[3]) for r1, r2, _ in validation_pairs]))\n",
    "print(compute_acc(train_clf(rm_arrays[4]), [extract_features(report_index[r1], report_index[r2], rm_arrays[4]) for r1, r2, _ in validation_pairs]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('inf8111')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0d4a9ff508ffe99659b2cf701ff6a706a654a039262bd7380280955940a4ce30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
